\section{Related Work}
Most of the relevant works selected from dblp\cite{dblp}, main criteria to choose were relevance, abstract API libraries and cloud management platforms evaluation and testbed design.

This topic has already raised repeatedly in the scientific community, for example, one of the complete works that are engaged in comparing existing abstract libraries is 'An Empirical Study for Evaluating the Performance of jclouds\cite{jclouds}'\cite{DBLP:conf/cloudcom/IsmaelSSR15} In this work, the primary focus is on the jclouds and performance results are compared with results from platform-specific libraries. The goal of this study is analysing jclouds API for evaluation concerning its performance against platform-specific APIs from a platform-specific API user interested in enabling cloud portability alternatively, multi-cloud management in the context of a Controlled Experiment. To achieve such a goal a 115 KB file is uploaded to the remote Amazon and Azure endpoints using the above-selected library and also the platform specific ones.  For the null hypothesis, it assumed that the download time for a file through the jclouds is the same as the download time through the platform-specific library. As a result, presented dependency graphs of the number of requests and uploading time.  In conclusion, pointed that the library's performance depends on the platform, in the experiments jclouds showed itself better compared to the AWS specific API but worse compared to the Azure specific API.

The same authors have expanded the work and called it  'An empirical study for evaluating the performance of multi-cloud APIs'\cite{DBLP:journals/fgcs/ReMJIS18}.  The goal of this study is to analyse two multi-cloud APIs for evaluation concerning their performance from the platform-specific API users in the context of uploading and downloading files to/from cloud blob storage services.  In contrast with the previous work, to the comparison added the libcloud\cite{libcloud} library, file sizes took the more significant range of sizes: 155KB, 310KB, 620KB, 1240KB, 2480KB. To the time evaluation criteria added the CPU time and KB memory criteria, in addition to download time also used uploading file time. This work was much more complete and covered much more problems. In the results, it concluded that the performance of multi-instrument clusters is strongly off-limits from the platform-specific libraries, jclouds is slightly worse in performance compared to platform-specific, while libcloud is better in most experiments. In multi-cloud library selection, the main effort should be on comparing particular attributes depending on the use case.

At the same conference was presented a work 'Critical evaluation on jClouds and Cloudify\cite{cloudify} abstract APIs against EC2, Azure and HP-Cloud'\cite{DBLP:conf/compsac/GrahamL14}, in this document the primary objectives were as follows:
\begin{itemize}
\item Analyze the problem and the current literature as well as ask questions that will form the basis for evaluating the abstract APIs. 
\item Create a tool for analysis of abstract APIs based on questions and criteria that highlighted in the current literature analysis. 
\item Create prototype tool that will evaluate jClouds and Cloudify. 
\end{itemize}
As a result, presented cloud evaluation tables comparing multi-cloud abstract APIs. Concluded that using abstract interfaces, most of the measured cloud criteria improved.

According to this papers, it is evident that this topic is very relevant and many try to solve it without a standardised test environment. In this work, we will offer our solution and architecture with the help of which it is possible to optimise and automate the evaluation performance tests.

Complete work for designing testbed for Grid\cite{DBLP:journals/ijhpca/BolzeCCDDJJLLMMNPQRTT06} describes highly configurable real-life experimental architecture that can be controlled and monitored directly. In this work considered large distributed systems, with numerous parameters and complex interactions between resources, make analytical modelling impractical.